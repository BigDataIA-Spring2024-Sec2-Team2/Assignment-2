{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb9d178-a25d-48ed-87d9-c09c4da6b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in e:\\spring 24\\big data\\assignnment 2\\assignment-2\\venv\\lib\\site-packages (3.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing and importing required libraries\n",
    "!pip install PyPDF2\n",
    "import PyPDF2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13830e75-78ff-470d-88da-33b9636695d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to process PDFs and extract topic outlines and learning outcomes\n",
    "def process_pdf(input_pdf_path, output_txt_path):\n",
    "    pdfFileObj = open(input_pdf_path, 'rb')\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "\n",
    "    print(f\"Processing {input_pdf_path}\")\n",
    "    print(\"Total number of pages:\", len(pdfReader.pages))\n",
    "    \n",
    "# Initializing dictionaries to store extracted content\n",
    "    content = dict()\n",
    "    topic = \"\"\n",
    "    topic_dict = dict()\n",
    "\n",
    "    # Iterating through each page of the PDF and extracting text from the current page and split into lines\n",
    "    for page_num in range(len(pdfReader.pages)):\n",
    "        t = pdfReader.pages[page_num].extract_text().split('\\n')\n",
    "        line_num = 0\n",
    "\n",
    "        # Skipping header lines containing 'topic outlines'\n",
    "        while line_num < len(t):\n",
    "            if line_num == 0:\n",
    "                if 'topic outlines' in t[line_num].strip().lower():\n",
    "                    line_num += 1\n",
    "                topic_new = re.sub(r'[^A-Za-z ]+', '', t[line_num]).strip()\n",
    "            \n",
    "                # Checking if the topic already exists in the content dictionary\n",
    "                all_keys = [x.lower().strip().replace(\" \", \"\") for x in content.keys()]\n",
    "                if topic_new.lower().strip().replace(\" \", \"\") in all_keys:\n",
    "                    topic_new = list(filter(lambda x: x.lower().strip().replace(\" \", \"\") == topic_new.lower().strip().replace(\" \", \"\"), content.keys()))[0]\n",
    "\n",
    "                # Updating the topic if it has changed\n",
    "                if topic == topic_new:\n",
    "                    pass\n",
    "                else:\n",
    "                    subtopic = \"\"\n",
    "                    subtopic_dict = []\n",
    "                    topic = topic_new\n",
    "            topic_dict = content.get(topic, dict())\n",
    "\n",
    "            # Identifying subtopics i.e. Learning outcoomes for Topics\n",
    "            subtopic_loc = t[line_num].find(\"The candidate should be able to:\")\n",
    "            if subtopic_loc != -1:\n",
    "                subtopic = t[line_num - 1] if subtopic_loc == 0 else t[line_num][:subtopic_loc + 1]\n",
    "                subtopic_dict = topic_dict.get(subtopic, [])\n",
    "                tab_loc = t[line_num].find(\"\\t\")\n",
    "\n",
    "                # Extract learning outcomes and appending to the subtopic dictionary\n",
    "                append_list = t[line_num][tab_loc + 1:] + t[line_num + 1]\n",
    "                if append_list.find(\"\\t\") == -1:\n",
    "                    subtopic_dict.append(append_list)\n",
    "\n",
    "                    line_num += 2\n",
    "                if line_num >= len(t):\n",
    "                    break\n",
    "            # Handeling corcer cases, extract learning outcomes from lines with tabs\n",
    "            tab_loc = t[line_num].find(\"\\t\")\n",
    "\n",
    "            if tab_loc != -1:\n",
    "                subtopic_dict.append(t[line_num][tab_loc + 1:])\n",
    "\n",
    "            # Updating the topic dictionary\n",
    "            topic_dict[subtopic] = subtopic_dict\n",
    "            content[topic] = topic_dict\n",
    "\n",
    "            line_num += 1\n",
    "    \n",
    "    # Writing the extracted data to an output text files\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as output_file:\n",
    "        for topic, subtopics in content.items():\n",
    "            output_file.write(f\"\\nTopic: {topic}\\n\")\n",
    "            output_file.write(\"\\t\\n\\tLearning Outcomes: \\n\")\n",
    "            output_file.write(\"\\t\\t(For the below Learning Outcomes, The candidate should be able to: )\\n\")\n",
    "            for subtopic, learning_outcomes in subtopics.items():\n",
    "                if subtopic == '':\n",
    "                    continue\n",
    "                output_file.write(f\"\\t{subtopic}\\n\")\n",
    "                for outcome in learning_outcomes:\n",
    "                    output_file.write(f\"\\t\\t- {outcome}\\n\")\n",
    "\n",
    "    print(f\"Output saved to: {output_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9186644c-8c76-49a4-bc09-780321ee0bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ..\\..\\data\\raw-pdf-data\\2024-l1-topics-combined-2.pdf\n",
      "Total number of pages: 27\n",
      "Output saved to: ..\\..\\data\\extracted-pdf-data_PyPDF2\\PyPDF_RR_2024_l1_combined.txt\n"
     ]
    }
   ],
   "source": [
    "# Processing all 3 PDF files for three different levels and storing extracted  data in 3 different text files\n",
    "process_pdf('..\\\\..\\\\data\\\\raw-pdf-data\\\\2024-l1-topics-combined-2.pdf', '..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR_2024_l1_combined.txt')\n",
    "process_pdf('..\\\\..\\\\data\\\\raw-pdf-data\\2024-l2-topics-combined-2.pdf', '..\\..\\data\\extracted-pdf-data_PyPDF2\\PyPDF_RR_2024_l2_combined.txt')\n",
    "process_pdf('..\\\\..\\\\data\\\\raw-pdf-data\\2024-l3-topics-combined-2.pdf', '..\\..\\data\\extracted-pdf-data_PyPDF2\\PyPDF_RR_2024_l3_combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd4c0b-f104-45a0-bf54-f0c9429d4977",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df512422",
   "metadata": {},
   "source": [
    "Grobid PDF Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "182c16f3-b0bf-4d9a-8da3-e92b5ad59883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grobid_client_python.grobid_client.grobid_client import GrobidClient\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15873e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs(input_directory, output_directory):\n",
    "    client = GrobidClient(config_path=\"./config.json\")\n",
    "    client.process(\"processFulltextDocument\", input_directory, output_directory, n=1, consolidate_header=False,consolidate_citations=True,force=True,segment_sentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b8d6bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_text(xml_string):\n",
    "    root = ET.fromstring(xml_string)\n",
    "    text = \"\"\n",
    "    for elem in root.iter():\n",
    "        if elem.text:\n",
    "            text += elem.text + \"\\n\"\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ea7fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROBID server is up and running\n",
      "../../data/extracted-pdf-data_Grobid/2024-l2-topics-combined-2.grobid.tei.xml\n",
      "Converted 2024-l2-topics-combined-2.grobid.tei.xml to Grobid_RR_2024_1_combined.txt\n",
      "../../data/extracted-pdf-data_Grobid/2024-l1-topics-combined-2.grobid.tei.xml\n",
      "Converted 2024-l1-topics-combined-2.grobid.tei.xml to Grobid_RR_2024_2_combined.txt\n",
      "../../data/extracted-pdf-data_Grobid/2024-l3-topics-combined-2.grobid.tei.xml\n",
      "Converted 2024-l3-topics-combined-2.grobid.tei.xml to Grobid_RR_2024_3_combined.txt\n"
     ]
    }
   ],
   "source": [
    "input_directory = \"../../data/raw-pdf-data\"  \n",
    "output_directory = \"../../data/extracted-pdf-data_Grobid/\"\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "process_pdfs(input_directory, output_directory)\n",
    "i=0\n",
    "for filename in os.listdir(output_directory):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        i+=1\n",
    "        xml_file_path = os.path.join(output_directory, filename)\n",
    "        print(xml_file_path)\n",
    "        with open(xml_file_path, \"r\", encoding=\"utf-8\") as xml_file:\n",
    "            xml_content = xml_file.read()\n",
    "            text_content = xml_to_text(xml_content)\n",
    "            txt_file_path = output_directory + \"Grobid_RR_\" + str(datetime.date.today().year) + \"_\" +  str(i)+ \"_combined\"+ \".txt\"\n",
    "            with open(txt_file_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
    "                txt_file.write(text_content)\n",
    "            print(f\"Converted {filename} to {os.path.basename(txt_file_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed3178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing XML files\n",
    "xml_directory = '../../data/extracted-pdf-data_Grobid'\n",
    "\n",
    "# Create a CSV file for storing metadata\n",
    "csv_file_path = '../../data/extracted-pdf-data_Grobid/grobid_metadata.csv'\n",
    "if os.path.exists(csv_file_path):\n",
    "    os.remove(csv_file_path)\n",
    "    \n",
    "csv_file = open(csv_file_path, 'w', newline='', encoding='utf-8')\n",
    "\n",
    "csv_writer = csv.writer(csv_file)\n",
    "\n",
    "# Write header row to CSV file\n",
    "csv_writer.writerow(['Filename','Title', 'Time','MD5 Identifier', 'Encoding Version','Lang', 'Application Identifier',\n",
    "                     'Application Description', 'Application Version', 'Application Reference URL'])\n",
    "\n",
    "\n",
    "# Iterate through XML files in the directory\n",
    "for filename in os.listdir(xml_directory):\n",
    "    if filename.endswith('.xml'):\n",
    "        # Parse the XML document\n",
    "        tree = ET.parse(os.path.join(xml_directory, filename))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Extract metadata from TEI header\n",
    "        title = root.find('.//{http://www.tei-c.org/ns/1.0}titleStmt/{http://www.tei-c.org/ns/1.0}title')\n",
    "        title = title.text if title is not None else \"\"\n",
    "\n",
    "        when_attribute = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]')\n",
    "        when_attribute = when_attribute.get('when') if when_attribute is not None else \"\"\n",
    "\n",
    "        md5_identifier = root.find('.//{http://www.tei-c.org/ns/1.0}sourceDesc/{http://www.tei-c.org/ns/1.0}biblStruct/{http://www.tei-c.org/ns/1.0}idno[@type=\"MD5\"]')\n",
    "        md5_identifier = md5_identifier.text if md5_identifier is not None else \"\"\n",
    "\n",
    "        version_attribute = root.get('encoding') if 'encoding' in root.attrib else \"UTF-8\"\n",
    "\n",
    "        lang_attribute = root.get('lang') if 'lang' in root.attrib else \"en\"\n",
    "\n",
    "        application_identifier = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]')\n",
    "        application_identifier = application_identifier.get('ident') if application_identifier is not None else \"\"\n",
    "\n",
    "        application_description = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]/{http://www.tei-c.org/ns/1.0}desc')\n",
    "        application_description = application_description.text if application_description is not None else \"\"\n",
    "\n",
    "        application_version = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]')\n",
    "        application_version = application_version.get('version') if application_version is not None else \"\"\n",
    "\n",
    "        application_reference_url = root.find('.//{http://www.tei-c.org/ns/1.0}appInfo/{http://www.tei-c.org/ns/1.0}application[@ident=\"GROBID\"]/{http://www.tei-c.org/ns/1.0}ref')\n",
    "        application_reference_url = application_reference_url.get('target') if application_reference_url is not None else \"\"\n",
    "\n",
    "        # Write metadata to CSV file\n",
    "        csv_writer.writerow([filename, title, when_attribute, md5_identifier, version_attribute, lang_attribute, application_identifier,\n",
    "                             application_description, application_version, application_reference_url])\n",
    "\n",
    "# Close CSV file\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa20569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
