{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b73e6a-2bac-4a94-9545-279c36d44a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7fe231f-e80e-42ea-8abe-ef15c9b7b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload files to S3\n",
    "def uploadFiletoS3(file_name, bucket, object_name):\n",
    "    try:  \n",
    "        # get S3 object\n",
    "        s3 = boto3.client('s3')\n",
    "\n",
    "        # upload file\n",
    "        with open(file_name, \"rb\") as f:\n",
    "             s3.upload_fileobj(f, bucket, object_name)\n",
    "    except ClientError as e:\n",
    "        print(\"Error in S3 Upload\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ea11b5-01e2-4e3e-bb0f-a92e013df07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload csv and log file for web scrapping to S3\n",
    "def upload_scrape_file():\n",
    "    # initialize source locations\n",
    "    data_source = \"..\\..\\data\\scrape-data\\cfa-data.csv\"\n",
    "    log_source = \"..\\..\\logs\\scrape-log\\webscrapping.log\"\n",
    "\n",
    "    # S3 bucket name\n",
    "    bucket = \"cfa-data-t2\"\n",
    "    \n",
    "    # initialize destination locations\n",
    "    data_destination = \"csv-files/cfa-data.csv\"\n",
    "    log_destination = \"log-files/webscrapping.log\"\n",
    "\n",
    "    # upload csv file\n",
    "    uploadFiletoS3(data_source, bucket, data_destination)\n",
    "\n",
    "    # upload log file\n",
    "    uploadFiletoS3(log_source, bucket, log_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7795127-75f8-4663-91bf-ff9d032a9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_extracted_data_files():\n",
    "    # initialize additional file locations\n",
    "    data_extracted_PyPDF2_source = \"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\\"\n",
    "    data_extracted_PyPDF2_destination = \"pypdf2-files/ \"\n",
    "    \n",
    "    data_extracted_Grobid_source = \"..\\\\..\\\\data\\\\extracted-pdf-data_Grobid\\\\\"\n",
    "    data_extracted_PyPDF2_destination = \" \"\n",
    "\n",
    "    # S3 bucket name\n",
    "    bucket = \"cfa-data-t2\"\n",
    "\n",
    "    # uploading extracted data files\n",
    "    uploadFiletoS3(data_extracted_PyPDF2_source, bucket, data_extracted_PyPDF2_destination)\n",
    "    uploadFiletoS3(data_extracted_Grobid_source, bucket, data_extracted_PyPDF2_destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774bc166-c1cf-4cc1-8a9f-68f7824d707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to upload files to upload text file for PDF data extraction using PyPDF2 and Grobid to S3\n",
    "def upload_extracted_data_files():\n",
    "\n",
    "    # List of file paths for PyPDF2 and Grobid\n",
    "    pyPDF2_files = [\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR__2024_levelI_combined.txt\", \"destination\": \"pypdf2-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR__2024_levelII_combined.txt\", \"destination\": \"pypdf2-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_PyPDF2\\\\PyPDF_RR__2024_levelIII_combined.txt\", \"destination\": \"pypdf2-files/\"},\n",
    "    ]\n",
    "    \n",
    "    grobid_files = [\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_Grobid\\\\Grobid_RR_2024_1_combined.txt\", \"destination\": \"grobid-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_Grobid\\\\Grobid_RR_2024_2_combined.txt\", \"destination\": \"grobid-files/\"},\n",
    "        {\"path\": \"..\\\\..\\\\data\\\\extracted-pdf-data_Grobid\\\\Grobid_RR_2024_3_combined.txt\", \"destination\": \"grobid-files/\"},\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # S3 bucket name\n",
    "    bucket = \"cfa-data-t2\"\n",
    "    \n",
    "    # Uploading extracted data files for PyPDF2\n",
    "    for file_info in pyPDF2_files:\n",
    "        uploadFiletoS3(file_info[\"path\"], bucket, file_info[\"destination\"] + file_info[\"path\"].split(\"\\\\\")[-1])\n",
    "    \n",
    "    # Uploading extracted data files for Grobid\n",
    "    for file_info in grobid_files:\n",
    "        uploadFiletoS3(file_info[\"path\"], bucket, file_info[\"destination\"] + file_info[\"path\"].split(\"\\\\\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108b4653-3a28-4877-975f-3a563599d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # upload data to S3\n",
    "    upload_scrape_file()\n",
    "    upload_extracted_data_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
